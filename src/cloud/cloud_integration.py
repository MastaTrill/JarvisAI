"""
‚òÅÔ∏è Cloud Integration and Auto-Scaling Module
==========================================

This module provides cloud platform integration and auto-scaling capabilities:
- AWS, Azure, GCP integration
- Auto-scaling based on workload
- Resource optimization
- Cost monitoring
- Serverless deployment
- Container orchestration

Author: Aetheron AI Platform
Version: 1.0.0
"""

import numpy as np
import pandas as pd
import time
import json
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
import threading

logger = logging.getLogger(__name__)

class CloudProvider(Enum):
    AWS = "aws"
    AZURE = "azure"
    GCP = "gcp"
    LOCAL = "local"

class ResourceType(Enum):
    CPU = "cpu"
    GPU = "gpu"
    MEMORY = "memory"
    STORAGE = "storage"

@dataclass
class ResourceSpec:
    """Resource specification for cloud instances"""
    cpu_cores: int
    memory_gb: int
    gpu_count: int = 0
    gpu_type: str = ""
    storage_gb: int = 100
    instance_type: str = ""
    cost_per_hour: float = 0.0

@dataclass
class ScalingPolicy:
    """Auto-scaling policy configuration"""
    min_instances: int = 1
    max_instances: int = 10
    target_cpu_utilization: float = 70.0
    target_memory_utilization: float = 80.0
    scale_up_threshold: float = 85.0
    scale_down_threshold: float = 30.0
    cooldown_period: int = 300  # seconds

class CloudManager:
    """Unified cloud platform management interface"""
    
    def __init__(self, provider: CloudProvider = CloudProvider.AWS):
        self.provider = provider
        self.instances = {}
        self.scaling_policies = {}
        self.cost_tracking = {}
        self.resource_usage = {}
        
        logger.info(f"‚òÅÔ∏è Initialized CloudManager for {provider.value}")
        
    def provision_resources(self, resource_spec: ResourceSpec, 
                          instance_count: int = 1) -> Dict[str, Any]:
        """
        Provision cloud resources based on specifications
        
        Args:
            resource_spec: Resource requirements
            instance_count: Number of instances to provision
            
        Returns:
            Provisioning results
        """
        try:
            logger.info(f"üöÄ Provisioning {instance_count} instances on {self.provider.value}")
            logger.info(f"üìã Spec: {resource_spec.cpu_cores} CPUs, {resource_spec.memory_gb}GB RAM, {resource_spec.gpu_count} GPUs")
            
            # Simulate cloud instance provisioning
            provision_time = np.random.uniform(30, 120)  # seconds
            time.sleep(provision_time / 30)  # Scaled for demo
            
            instances = []
            total_cost = 0.0
            
            for i in range(instance_count):
                instance_id = f"{self.provider.value}-instance-{int(time.time())}-{i}"
                
                # Simulate instance details
                instance = {
                    'instance_id': instance_id,
                    'provider': self.provider.value,
                    'resource_spec': asdict(resource_spec),
                    'status': 'running',
                    'launch_time': time.time(),
                    'public_ip': f"203.0.113.{np.random.randint(1, 255)}",
                    'private_ip': f"10.0.1.{np.random.randint(1, 255)}",
                    'region': self._get_optimal_region(),
                    'cost_per_hour': resource_spec.cost_per_hour or self._estimate_cost(resource_spec)
                }
                
                instances.append(instance)
                self.instances[instance_id] = instance
                total_cost += instance['cost_per_hour']
            
            result = {
                'success': True,
                'instances': instances,
                'instance_count': len(instances),
                'total_cost_per_hour': total_cost,
                'provision_time': provision_time,
                'estimated_monthly_cost': total_cost * 24 * 30
            }
            
            logger.info(f"‚úÖ Successfully provisioned {len(instances)} instances")
            logger.info(f"üí∞ Cost: ${total_cost:.2f}/hour (${total_cost * 24 * 30:.2f}/month)")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error provisioning resources: {e}")
            return {'success': False, 'error': str(e)}
    
    def _get_optimal_region(self) -> str:
        """Get optimal region based on provider"""
        regions = {
            CloudProvider.AWS: ['us-east-1', 'us-west-2', 'eu-west-1', 'ap-southeast-1'],
            CloudProvider.AZURE: ['East US', 'West Europe', 'Southeast Asia', 'Australia East'],
            CloudProvider.GCP: ['us-central1', 'europe-west1', 'asia-southeast1', 'australia-southeast1']
        }
        
        available_regions = regions.get(self.provider, ['local'])
        return np.random.choice(available_regions)
    
    def _estimate_cost(self, spec: ResourceSpec) -> float:
        """Estimate hourly cost based on resource specification"""
        base_costs = {
            CloudProvider.AWS: {'cpu': 0.05, 'memory': 0.01, 'gpu': 1.0},
            CloudProvider.AZURE: {'cpu': 0.048, 'memory': 0.009, 'gpu': 0.95},
            CloudProvider.GCP: {'cpu': 0.047, 'memory': 0.008, 'gpu': 0.90}
        }
        
        costs = base_costs.get(self.provider, {'cpu': 0.05, 'memory': 0.01, 'gpu': 1.0})
        
        total_cost = (
            spec.cpu_cores * costs['cpu'] +
            spec.memory_gb * costs['memory'] +
            spec.gpu_count * costs['gpu']
        )
        
        return round(total_cost, 3)
    
    def setup_auto_scaling(self, scaling_policy: ScalingPolicy) -> Dict[str, Any]:
        """
        Configure auto-scaling for the deployment
        
        Args:
            scaling_policy: Auto-scaling configuration
            
        Returns:
            Auto-scaling setup results
        """
        try:
            logger.info("üîÑ Setting up auto-scaling...")
            logger.info(f"üìä Policy: {scaling_policy.min_instances}-{scaling_policy.max_instances} instances")
            logger.info(f"üéØ Target CPU: {scaling_policy.target_cpu_utilization}%")
            
            policy_id = f"policy-{int(time.time())}"
            
            self.scaling_policies[policy_id] = {
                'policy_id': policy_id,
                'config': asdict(scaling_policy),
                'status': 'active',
                'created_at': time.time(),
                'scaling_events': []
            }
            
            # Start monitoring thread (simulated)
            monitoring_thread = threading.Thread(
                target=self._monitor_and_scale,
                args=(policy_id, scaling_policy),
                daemon=True
            )
            monitoring_thread.start()
            
            result = {
                'success': True,
                'policy_id': policy_id,
                'monitoring_interval': 60,  # seconds
                'scaling_policy': asdict(scaling_policy)
            }
            
            logger.info(f"‚úÖ Auto-scaling configured with policy ID: {policy_id}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error setting up auto-scaling: {e}")
            return {'success': False, 'error': str(e)}
    
    def _monitor_and_scale(self, policy_id: str, policy: ScalingPolicy):
        """Background monitoring and scaling (simulated)"""
        while policy_id in self.scaling_policies:
            try:
                time.sleep(10)  # Monitoring interval (scaled for demo)
                
                # Simulate resource usage
                current_instances = len([i for i in self.instances.values() if i['status'] == 'running'])
                cpu_usage = np.random.uniform(20, 95)
                memory_usage = np.random.uniform(30, 90)
                
                self.resource_usage[policy_id] = {
                    'timestamp': time.time(),
                    'instances': current_instances,
                    'cpu_usage': cpu_usage,
                    'memory_usage': memory_usage
                }
                
                # Scaling decisions
                should_scale_up = (
                    cpu_usage > policy.scale_up_threshold or
                    memory_usage > policy.scale_up_threshold
                ) and current_instances < policy.max_instances
                
                should_scale_down = (
                    cpu_usage < policy.scale_down_threshold and
                    memory_usage < policy.scale_down_threshold
                ) and current_instances > policy.min_instances
                
                if should_scale_up:
                    self._scale_up(policy_id, policy)
                elif should_scale_down:
                    self._scale_down(policy_id, policy)
                    
            except Exception as e:
                logger.error(f"‚ùå Error in monitoring thread: {e}")
                break
    
    def _scale_up(self, policy_id: str, policy: ScalingPolicy):
        """Scale up instances"""
        logger.info("üìà Scaling up instances...")
        
        # Add scaling event
        scaling_event = {
            'timestamp': time.time(),
            'action': 'scale_up',
            'reason': 'high_resource_usage',
            'instances_before': len(self.instances),
            'instances_after': len(self.instances) + 1
        }
        
        self.scaling_policies[policy_id]['scaling_events'].append(scaling_event)
        
        # Simulate adding instance
        new_instance_id = f"scaled-{int(time.time())}"
        self.instances[new_instance_id] = {
            'instance_id': new_instance_id,
            'status': 'running',
            'scaled': True,
            'created_by_autoscaler': True
        }
        
        logger.info(f"‚úÖ Scaled up: {scaling_event['instances_after']} instances")
    
    def _scale_down(self, policy_id: str, policy: ScalingPolicy):
        """Scale down instances"""
        logger.info("üìâ Scaling down instances...")
        
        # Find instance to terminate
        scalable_instances = [
            id for id, instance in self.instances.items()
            if instance.get('scaled', False) and instance['status'] == 'running'
        ]
        
        if scalable_instances:
            instance_to_remove = scalable_instances[0]
            self.instances[instance_to_remove]['status'] = 'terminated'
            
            scaling_event = {
                'timestamp': time.time(),
                'action': 'scale_down',
                'reason': 'low_resource_usage',
                'terminated_instance': instance_to_remove,
                'instances_after': len([i for i in self.instances.values() if i['status'] == 'running'])
            }
            
            self.scaling_policies[policy_id]['scaling_events'].append(scaling_event)
            logger.info(f"‚úÖ Scaled down: {scaling_event['instances_after']} instances")

class ServerlessDeployer:
    """Serverless deployment management for ML models"""
    
    def __init__(self, provider: CloudProvider = CloudProvider.AWS):
        self.provider = provider
        self.functions = {}
        self.api_gateways = {}
        
        logger.info(f"‚ö° Initialized ServerlessDeployer for {provider.value}")
    
    def deploy_model_function(self, model_path: str, function_name: str,
                            runtime: str = "python3.9", memory_mb: int = 512,
                            timeout_seconds: int = 30) -> Dict[str, Any]:
        """
        Deploy ML model as serverless function
        
        Args:
            model_path: Path to serialized model
            function_name: Name for the serverless function
            runtime: Runtime environment
            memory_mb: Memory allocation in MB
            timeout_seconds: Function timeout
            
        Returns:
            Deployment results
        """
        try:
            logger.info(f"‚ö° Deploying {function_name} as serverless function...")
            logger.info(f"üîß Runtime: {runtime}, Memory: {memory_mb}MB, Timeout: {timeout_seconds}s")
            
            # Simulate serverless deployment
            deployment_time = np.random.uniform(10, 30)
            time.sleep(deployment_time / 10)  # Scaled for demo
            
            function_arn = f"arn:{self.provider.value}:lambda:us-east-1:123456789012:function:{function_name}"
            endpoint_url = f"https://api.{self.provider.value}.com/v1/{function_name}"
            
            function_config = {
                'function_name': function_name,
                'function_arn': function_arn,
                'runtime': runtime,
                'memory_mb': memory_mb,
                'timeout_seconds': timeout_seconds,
                'model_path': model_path,
                'endpoint_url': endpoint_url,
                'deployment_time': deployment_time,
                'status': 'active',
                'created_at': time.time(),
                'invocation_count': 0,
                'cold_starts': 0,
                'estimated_cost_per_invocation': self._calculate_serverless_cost(memory_mb, timeout_seconds)
            }
            
            self.functions[function_name] = function_config
            
            result = {
                'success': True,
                'function_config': function_config,
                'deployment_time': deployment_time,
                'endpoint_url': endpoint_url
            }
            
            logger.info(f"‚úÖ Function deployed successfully")
            logger.info(f"üåê Endpoint: {endpoint_url}")
            logger.info(f"üí∞ Cost per invocation: ${function_config['estimated_cost_per_invocation']:.6f}")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error deploying serverless function: {e}")
            return {'success': False, 'error': str(e)}
    
    def _calculate_serverless_cost(self, memory_mb: int, timeout_seconds: int) -> float:
        """Calculate estimated cost per serverless invocation"""
        # Simplified cost calculation
        gb_seconds = (memory_mb / 1024) * timeout_seconds
        
        cost_per_gb_second = {
            CloudProvider.AWS: 0.0000166667,
            CloudProvider.AZURE: 0.000016,
            CloudProvider.GCP: 0.0000025
        }
        
        base_cost = cost_per_gb_second.get(self.provider, 0.0000166667)
        request_cost = 0.0000002  # Per request
        
        return gb_seconds * base_cost + request_cost
    
    def setup_api_gateway(self, function_name: str, api_name: str) -> Dict[str, Any]:
        """
        Setup API Gateway for serverless function
        
        Args:
            function_name: Name of the serverless function
            api_name: Name for the API Gateway
            
        Returns:
            API Gateway configuration
        """
        try:
            logger.info(f"üåê Setting up API Gateway: {api_name}")
            
            if function_name not in self.functions:
                raise ValueError(f"Function {function_name} not found")
            
            api_id = f"api-{int(time.time())}"
            api_url = f"https://{api_id}.execute-api.us-east-1.{self.provider.value}.com/prod"
            
            api_config = {
                'api_id': api_id,
                'api_name': api_name,
                'function_name': function_name,
                'api_url': api_url,
                'methods': ['GET', 'POST'],
                'cors_enabled': True,
                'rate_limiting': {
                    'requests_per_second': 1000,
                    'burst_limit': 5000
                },
                'monitoring_enabled': True,
                'caching_enabled': False,
                'created_at': time.time()
            }
            
            self.api_gateways[api_name] = api_config
            
            logger.info(f"‚úÖ API Gateway configured")
            logger.info(f"üîó API URL: {api_url}")
            
            return {
                'success': True,
                'api_config': api_config
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error setting up API Gateway: {e}")
            return {'success': False, 'error': str(e)}
    
    def simulate_function_invocations(self, function_name: str, 
                                    invocation_count: int = 100) -> Dict[str, Any]:
        """
        Simulate serverless function invocations for testing
        
        Args:
            function_name: Name of function to invoke
            invocation_count: Number of invocations to simulate
            
        Returns:
            Invocation results and statistics
        """
        try:
            logger.info(f"üß™ Simulating {invocation_count} invocations for {function_name}")
            
            if function_name not in self.functions:
                raise ValueError(f"Function {function_name} not found")
            
            function = self.functions[function_name]
            
            # Simulate invocations
            response_times = []
            cold_starts = 0
            errors = 0
            
            for i in range(invocation_count):
                # Simulate cold start probability
                is_cold_start = np.random.random() < 0.05  # 5% cold start rate
                
                if is_cold_start:
                    response_time = np.random.uniform(500, 2000)  # ms
                    cold_starts += 1
                else:
                    response_time = np.random.uniform(50, 300)  # ms
                
                # Simulate errors
                if np.random.random() < 0.01:  # 1% error rate
                    errors += 1
                    
                response_times.append(response_time)
            
            # Update function statistics
            function['invocation_count'] += invocation_count
            function['cold_starts'] += cold_starts
            
            # Calculate statistics
            stats = {
                'total_invocations': invocation_count,
                'successful_invocations': invocation_count - errors,
                'failed_invocations': errors,
                'cold_starts': cold_starts,
                'success_rate': (invocation_count - errors) / invocation_count * 100,
                'cold_start_rate': cold_starts / invocation_count * 100,
                'average_response_time': np.mean(response_times),
                'p95_response_time': np.percentile(response_times, 95),
                'p99_response_time': np.percentile(response_times, 99),
                'total_cost': invocation_count * function['estimated_cost_per_invocation']
            }
            
            logger.info(f"‚úÖ Simulation completed")
            logger.info(f"üìä Success rate: {stats['success_rate']:.1f}%")
            logger.info(f"‚ùÑÔ∏è Cold start rate: {stats['cold_start_rate']:.1f}%")
            logger.info(f"‚è±Ô∏è Average response time: {stats['average_response_time']:.1f}ms")
            logger.info(f"üí∞ Total cost: ${stats['total_cost']:.4f}")
            
            return {
                'success': True,
                'function_name': function_name,
                'statistics': stats
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error simulating invocations: {e}")
            return {'success': False, 'error': str(e)}

class CostOptimizer:
    """Cloud cost optimization and monitoring"""
    
    def __init__(self):
        self.cost_history = {}
        self.optimization_rules = []
        
    def analyze_costs(self, cloud_manager: CloudManager) -> Dict[str, Any]:
        """
        Analyze cloud costs and provide optimization recommendations
        
        Args:
            cloud_manager: CloudManager instance to analyze
            
        Returns:
            Cost analysis and recommendations
        """
        try:
            logger.info("üí∞ Analyzing cloud costs...")
            
            total_cost = 0.0
            instance_costs = []
            
            for instance in cloud_manager.instances.values():
                if instance['status'] == 'running':
                    hourly_cost = instance.get('cost_per_hour', 0.0)
                    daily_cost = hourly_cost * 24
                    monthly_cost = daily_cost * 30
                    
                    instance_costs.append({
                        'instance_id': instance['instance_id'],
                        'hourly_cost': hourly_cost,
                        'daily_cost': daily_cost,
                        'monthly_cost': monthly_cost,
                        'utilization': np.random.uniform(30, 90)  # Simulated
                    })
                    
                    total_cost += hourly_cost
            
            # Generate optimization recommendations
            recommendations = []
            
            # Check for underutilized instances
            underutilized = [i for i in instance_costs if i['utilization'] < 40]
            if underutilized:
                potential_savings = sum(i['monthly_cost'] for i in underutilized) * 0.7
                recommendations.append({
                    'type': 'rightsizing',
                    'description': f'Downsize {len(underutilized)} underutilized instances',
                    'potential_monthly_savings': potential_savings,
                    'affected_instances': len(underutilized)
                })
            
            # Check for reserved instance opportunities
            if len(instance_costs) >= 3:
                potential_savings = total_cost * 24 * 30 * 0.3  # 30% savings with reserved instances
                recommendations.append({
                    'type': 'reserved_instances',
                    'description': 'Use reserved instances for consistent workloads',
                    'potential_monthly_savings': potential_savings,
                    'commitment_required': '1-3 years'
                })
            
            # Check for spot instance opportunities
            spot_eligible = [i for i in instance_costs if i['utilization'] > 60]
            if spot_eligible:
                potential_savings = sum(i['monthly_cost'] for i in spot_eligible) * 0.6  # 60% savings
                recommendations.append({
                    'type': 'spot_instances',
                    'description': f'Use spot instances for {len(spot_eligible)} fault-tolerant workloads',
                    'potential_monthly_savings': potential_savings,
                    'risk': 'Instance interruption possible'
                })
            
            analysis = {
                'current_costs': {
                    'hourly_total': total_cost,
                    'daily_total': total_cost * 24,
                    'monthly_total': total_cost * 24 * 30,
                    'yearly_total': total_cost * 24 * 365
                },
                'instance_breakdown': instance_costs,
                'optimization_recommendations': recommendations,
                'potential_total_savings': sum(r['potential_monthly_savings'] for r in recommendations),
                'cost_efficiency_score': self._calculate_efficiency_score(instance_costs)
            }
            
            logger.info(f"üìä Current monthly cost: ${analysis['current_costs']['monthly_total']:.2f}")
            logger.info(f"üí° Potential monthly savings: ${analysis['potential_total_savings']:.2f}")
            logger.info(f"‚≠ê Cost efficiency score: {analysis['cost_efficiency_score']:.1f}/100")
            
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå Error analyzing costs: {e}")
            return {'error': str(e)}
    
    def _calculate_efficiency_score(self, instance_costs: List[Dict]) -> float:
        """Calculate cost efficiency score based on utilization"""
        if not instance_costs:
            return 0.0
        
        avg_utilization = np.mean([i['utilization'] for i in instance_costs])
        
        # Score based on utilization efficiency
        if avg_utilization >= 80:
            return 95.0
        elif avg_utilization >= 60:
            return 80.0
        elif avg_utilization >= 40:
            return 60.0
        else:
            return 30.0

# Utility functions
def get_recommended_instance_type(workload_type: str, provider: CloudProvider) -> ResourceSpec:
    """
    Get recommended instance type based on workload
    
    Args:
        workload_type: Type of ML workload ('training', 'inference', 'data_processing')
        provider: Cloud provider
        
    Returns:
        Recommended resource specification
    """
    recommendations = {
        'training': {
            CloudProvider.AWS: ResourceSpec(cpu_cores=8, memory_gb=32, gpu_count=1, gpu_type="V100", instance_type="p3.2xlarge"),
            CloudProvider.AZURE: ResourceSpec(cpu_cores=8, memory_gb=32, gpu_count=1, gpu_type="V100", instance_type="Standard_NC6s_v3"),
            CloudProvider.GCP: ResourceSpec(cpu_cores=8, memory_gb=30, gpu_count=1, gpu_type="V100", instance_type="n1-standard-8")
        },
        'inference': {
            CloudProvider.AWS: ResourceSpec(cpu_cores=4, memory_gb=16, instance_type="c5.xlarge"),
            CloudProvider.AZURE: ResourceSpec(cpu_cores=4, memory_gb=16, instance_type="Standard_F4s_v2"),
            CloudProvider.GCP: ResourceSpec(cpu_cores=4, memory_gb=15, instance_type="n1-standard-4")
        },
        'data_processing': {
            CloudProvider.AWS: ResourceSpec(cpu_cores=16, memory_gb=64, instance_type="r5.4xlarge"),
            CloudProvider.AZURE: ResourceSpec(cpu_cores=16, memory_gb=64, instance_type="Standard_E16s_v3"),
            CloudProvider.GCP: ResourceSpec(cpu_cores=16, memory_gb=60, instance_type="n1-highmem-16")
        }
    }
    
    provider_recs = recommendations.get(workload_type, recommendations['inference'])
    return provider_recs.get(provider, provider_recs[CloudProvider.AWS])
