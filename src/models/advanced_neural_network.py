"""
Advanced Neural Network Architectures for Jarvis AI.

This module provides enhanced neural network implementations with modern features:
- Regularization techniques (Dropout, BatchNorm, L1/L2)
- Different activation functions
- Advanced optimizers
- Ensemble methods
"""

import numpy as np
import logging
from typing import List, Optional, Callable, Dict, Any
from abc import ABC, abstractmethod
import pickle
import json

logger = logging.getLogger(__name__)


class ActivationFunction:
    """Collection of activation functions and their derivatives."""
    
    @staticmethod
    def relu(x: np.ndarray) -> np.ndarray:
        return np.maximum(0, x)
    
    @staticmethod
    def relu_derivative(x: np.ndarray) -> np.ndarray:
        return (x > 0).astype(float)
    
    @staticmethod
    def leaky_relu(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:
        return np.where(x > 0, x, alpha * x)
    
    @staticmethod
    def leaky_relu_derivative(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:
        return np.where(x > 0, 1.0, alpha)
    
    @staticmethod
    def sigmoid(x: np.ndarray) -> np.ndarray:
        # Clip to prevent overflow
        x = np.clip(x, -500, 500)
        return 1 / (1 + np.exp(-x))
    
    @staticmethod
    def sigmoid_derivative(x: np.ndarray) -> np.ndarray:
        s = ActivationFunction.sigmoid(x)
        return s * (1 - s)
    
    @staticmethod
    def tanh(x: np.ndarray) -> np.ndarray:
        return np.tanh(x)
    
    @staticmethod
    def tanh_derivative(x: np.ndarray) -> np.ndarray:
        return 1 - np.tanh(x) ** 2
    
    @staticmethod
    def swish(x: np.ndarray) -> np.ndarray:
        return x * ActivationFunction.sigmoid(x)
    
    @staticmethod
    def swish_derivative(x: np.ndarray) -> np.ndarray:
        s = ActivationFunction.sigmoid(x)
        return s + x * s * (1 - s)
    
    @staticmethod
    def softmax(x: np.ndarray) -> np.ndarray:
        """Softmax activation function (numerically stable)."""
        # Subtract max for numerical stability
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    @staticmethod
    def softmax_derivative(x: np.ndarray) -> np.ndarray:
        """Softmax derivative (for classification)."""
        s = ActivationFunction.softmax(x)
        return s * (1 - s)
    
    @staticmethod
    def gelu(x: np.ndarray) -> np.ndarray:
        """GELU activation function."""
        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
    
    @staticmethod
    def gelu_derivative(x: np.ndarray) -> np.ndarray:
        """GELU derivative."""
        cdf = 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
        pdf = np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)
        return cdf + x * pdf


class Regularizer:
    """Regularization techniques for neural networks."""
    
    @staticmethod
    def l1_penalty(weights: np.ndarray, lambda_reg: float) -> float:
        """L1 regularization penalty."""
        return lambda_reg * np.sum(np.abs(weights))
    
    @staticmethod
    def l2_penalty(weights: np.ndarray, lambda_reg: float) -> float:
        """L2 regularization penalty."""
        return lambda_reg * np.sum(weights ** 2)
    
    @staticmethod
    def l1_gradient(weights: np.ndarray, lambda_reg: float) -> np.ndarray:
        """L1 regularization gradient."""
        return lambda_reg * np.sign(weights)
    
    @staticmethod
    def l2_gradient(weights: np.ndarray, lambda_reg: float) -> np.ndarray:
        """L2 regularization gradient."""
        return 2 * lambda_reg * weights


class Optimizer:
    """Advanced optimizers for neural network training."""
    
    def __init__(self, learning_rate: float = 0.001):
        self.learning_rate = learning_rate
    
    @abstractmethod
    def update(self, weights: np.ndarray, gradients: np.ndarray) -> np.ndarray:
        pass


class SGDOptimizer(Optimizer):
    """Stochastic Gradient Descent optimizer."""
    
    def __init__(self, learning_rate: float = 0.001, momentum: float = 0.0):
        super().__init__(learning_rate)
        self.momentum = momentum
        self.velocity = None
    
    def update(self, weights: np.ndarray, gradients: np.ndarray) -> np.ndarray:
        if self.velocity is None:
            self.velocity = np.zeros_like(weights)
        
        self.velocity = self.momentum * self.velocity - self.learning_rate * gradients
        return weights + self.velocity


class AdamOptimizer(Optimizer):
    """Adam optimizer implementation."""
    
    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, 
                 beta2: float = 0.999, epsilon: float = 1e-8):
        super().__init__(learning_rate)
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None
        self.v = None
        self.t = 0
    
    def update(self, weights: np.ndarray, gradients: np.ndarray) -> np.ndarray:
        if self.m is None:
            self.m = np.zeros_like(weights)
            self.v = np.zeros_like(weights)
        
        self.t += 1
        
        # Update biased first moment estimate
        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients
        
        # Update biased second raw moment estimate
        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)
        
        # Compute bias-corrected first moment estimate
        m_hat = self.m / (1 - self.beta1 ** self.t)
        
        # Compute bias-corrected second raw moment estimate
        v_hat = self.v / (1 - self.beta2 ** self.t)
        
        # Update weights
        return weights - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)


class AdvancedNeuralNetwork:
    """
    Advanced Neural Network with regularization, dropout, and modern optimizers.
    """
    
    def __init__(
        self,
        input_size: int,
        hidden_sizes: List[int],
        output_size: int,
        activation: str = 'relu',
        output_activation: str = 'linear',
        dropout_rate: float = 0.0,
        l1_reg: float = 0.0,
        l2_reg: float = 0.0,
        optimizer: str = 'adam',
        learning_rate: float = 0.001,
        random_seed: Optional[int] = None
    ):
        """
        Initialize advanced neural network.
        
        Args:
            input_size: Number of input features
            hidden_sizes: List of hidden layer sizes
            output_size: Number of output neurons
            activation: Activation function ('relu', 'sigmoid', 'tanh', 'leaky_relu', 'swish')
            output_activation: Output activation function
            dropout_rate: Dropout rate for regularization
            l1_reg: L1 regularization strength
            l2_reg: L2 regularization strength
            optimizer: Optimizer type ('sgd', 'adam')
            learning_rate: Learning rate
            random_seed: Random seed for reproducibility
        """
        if random_seed is not None:
            np.random.seed(random_seed)
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size
        self.dropout_rate = dropout_rate
        self.l1_reg = l1_reg
        self.l2_reg = l2_reg
        self.is_trained = False
        
        # Set activation functions
        self.activation_name = activation
        self.output_activation_name = output_activation
        self._set_activation_functions()
        
        # Initialize optimizer
        self._initialize_optimizer(optimizer, learning_rate)
        
        # Initialize network architecture
        self._initialize_weights()
        
        logger.info(f"Advanced Neural Network initialized with architecture: "
                   f"{input_size} -> {' -> '.join(map(str, hidden_sizes))} -> {output_size}")
        logger.info(f"Activation: {activation}, Dropout: {dropout_rate}, "
                   f"L1: {l1_reg}, L2: {l2_reg}, Optimizer: {optimizer}")
    
    def _set_activation_functions(self):
        """Set activation functions based on string names."""
        activation_map = {
            'relu': (ActivationFunction.relu, ActivationFunction.relu_derivative),
            'sigmoid': (ActivationFunction.sigmoid, ActivationFunction.sigmoid_derivative),
            'tanh': (ActivationFunction.tanh, ActivationFunction.tanh_derivative),
            'leaky_relu': (ActivationFunction.leaky_relu, ActivationFunction.leaky_relu_derivative),
            'swish': (ActivationFunction.swish, ActivationFunction.swish_derivative),
            'softmax': (ActivationFunction.softmax, ActivationFunction.softmax_derivative),
            'gelu': (ActivationFunction.gelu, ActivationFunction.gelu_derivative),
            'linear': (lambda x: x, lambda x: np.ones_like(x))
        }
        
        self.activation_func, self.activation_derivative = activation_map[self.activation_name]
        self.output_activation_func, self.output_activation_derivative = activation_map[self.output_activation_name]
    
    def _initialize_optimizer(self, optimizer: str, learning_rate: float):
        """Initialize the optimizer."""
        if optimizer == 'sgd':
            # Create separate optimizer instances for each layer
            self.optimizers = []
            for _ in range(len([self.input_size] + self.hidden_sizes + [self.output_size]) - 1):
                self.optimizers.extend([
                    SGDOptimizer(learning_rate=learning_rate, momentum=0.9),  # for weights
                    SGDOptimizer(learning_rate=learning_rate, momentum=0.9)   # for biases
                ])
        elif optimizer == 'adam':
            # Create separate optimizer instances for each layer
            self.optimizers = []
            for _ in range(len([self.input_size] + self.hidden_sizes + [self.output_size]) - 1):
                self.optimizers.extend([
                    AdamOptimizer(learning_rate=learning_rate),  # for weights
                    AdamOptimizer(learning_rate=learning_rate)   # for biases
                ])
        else:
            raise ValueError(f"Unknown optimizer: {optimizer}")
    
    def _initialize_weights(self):
        """Initialize network weights using Xavier/Glorot initialization."""
        self.weights = []
        self.biases = []
        
        # Input to first hidden layer
        layer_sizes = [self.input_size] + self.hidden_sizes + [self.output_size]
        
        for i in range(len(layer_sizes) - 1):
            # Xavier initialization
            limit = np.sqrt(6.0 / (layer_sizes[i] + layer_sizes[i + 1]))
            weight = np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i + 1]))
            bias = np.zeros((1, layer_sizes[i + 1]))
            
            self.weights.append(weight)
            self.biases.append(bias)
    
    def _apply_dropout(self, x: np.ndarray, training: bool = True) -> np.ndarray:
        """Apply dropout during training."""
        if training and self.dropout_rate > 0:
            dropout_mask = np.random.random(x.shape) > self.dropout_rate
            return x * dropout_mask / (1 - self.dropout_rate)
        return x
    
    def forward(self, X: np.ndarray, training: bool = False) -> tuple:
        """
        Forward propagation with dropout support.
        
        Returns:
            output, activations for backpropagation
        """
        activations = [X]
        current_input = X
        
        # Hidden layers
        for i in range(len(self.weights) - 1):
            z = np.dot(current_input, self.weights[i]) + self.biases[i]
            current_input = self.activation_func(z)
            current_input = self._apply_dropout(current_input, training)
            activations.append(current_input)
        
        # Output layer
        z_output = np.dot(current_input, self.weights[-1]) + self.biases[-1]
        output = self.output_activation_func(z_output)
        activations.append(output)
        
        return output, activations
    
    def _compute_regularization_loss(self) -> float:
        """Compute regularization loss."""
        reg_loss = 0.0
        for weight in self.weights:
            if self.l1_reg > 0:
                reg_loss += Regularizer.l1_penalty(weight, self.l1_reg)
            if self.l2_reg > 0:
                reg_loss += Regularizer.l2_penalty(weight, self.l2_reg)
        return reg_loss
    
    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        epochs: int = 100,
        batch_size: int = 32,
        validation_data: Optional[tuple] = None,
        verbose: bool = True
    ) -> Dict[str, List[float]]:
        """
        Train the neural network with advanced features.
        
        Returns:
            Dictionary containing training history
        """
        if len(y.shape) == 1:
            y = y.reshape(-1, 1)
        
        history = {'train_loss': [], 'train_mse': []}
        if validation_data is not None:
            history['val_loss'] = []
            history['val_mse'] = []
        
        n_samples = X.shape[0]
        
        for epoch in range(epochs):
            # Shuffle data
            indices = np.random.permutation(n_samples)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            epoch_loss = 0.0
            epoch_mse = 0.0
            
            # Mini-batch training
            for i in range(0, n_samples, batch_size):
                batch_X = X_shuffled[i:i + batch_size]
                batch_y = y_shuffled[i:i + batch_size]
                
                # Forward pass
                predictions, activations = self.forward(batch_X, training=True)
                
                # Compute loss
                mse_loss = np.mean((predictions - batch_y) ** 2)
                reg_loss = self._compute_regularization_loss()
                total_loss = mse_loss + reg_loss
                
                epoch_loss += total_loss
                epoch_mse += mse_loss
                
                # Backward pass
                self._backward(batch_X, batch_y, predictions, activations)
            
            # Average losses
            epoch_loss /= (n_samples // batch_size + 1)
            epoch_mse /= (n_samples // batch_size + 1)
            
            history['train_loss'].append(epoch_loss)
            history['train_mse'].append(epoch_mse)
            
            # Validation
            if validation_data is not None:
                X_val, y_val = validation_data
                if len(y_val.shape) == 1:
                    y_val = y_val.reshape(-1, 1)
                val_pred, _ = self.forward(X_val, training=False)
                val_mse = np.mean((val_pred - y_val) ** 2)
                val_loss = val_mse + self._compute_regularization_loss()
                
                history['val_loss'].append(val_loss)
                history['val_mse'].append(val_mse)
            else:
                val_loss = 0.0
                val_mse = 0.0
            
            # Logging
            if verbose and epoch % 20 == 0:
                log_msg = f"Epoch {epoch}, Train Loss: {epoch_loss:.6f}, Train MSE: {epoch_mse:.6f}"
                if validation_data is not None:
                    log_msg += f", Val Loss: {val_loss:.6f}, Val MSE: {val_mse:.6f}"
                logger.info(log_msg)
        
        self.is_trained = True
        logger.info("Training completed successfully!")
        return history
    
    def _backward(self, X: np.ndarray, y: np.ndarray, predictions: np.ndarray, activations: List[np.ndarray]):
        """Backward propagation with regularization."""
        batch_size = X.shape[0]
        
        # Ensure y has the correct shape
        if len(y.shape) == 1:
            y = y.reshape(-1, 1)
        
        # Output layer error
        error = predictions - y
        
        # Backpropagate through layers
        for i in range(len(self.weights) - 1, -1, -1):
            # Compute gradients
            if i == len(self.weights) - 1:
                # Output layer
                weight_gradient = np.dot(activations[i].T, error) / batch_size
            else:
                # Hidden layers - apply activation derivative to current layer activations
                # Get the pre-activation values (z) for derivative calculation
                z = activations[i + 1]  # This contains the activated values
                if i < len(self.weights) - 2:  # Not the last hidden layer
                    activation_grad = self.activation_derivative(z)
                else:  # Last hidden layer before output
                    activation_grad = self.activation_derivative(z)
                
                error = np.dot(error, self.weights[i + 1].T) * activation_grad
                weight_gradient = np.dot(activations[i].T, error) / batch_size
            
            bias_gradient = np.mean(error, axis=0, keepdims=True)
            
            # Add regularization gradients
            if self.l1_reg > 0:
                weight_gradient += Regularizer.l1_gradient(self.weights[i], self.l1_reg)
            if self.l2_reg > 0:
                weight_gradient += Regularizer.l2_gradient(self.weights[i], self.l2_reg)
            
            # Update weights using separate optimizers for each layer
            weight_optimizer_idx = i * 2      # Even indices for weights
            bias_optimizer_idx = i * 2 + 1    # Odd indices for biases
            
            self.weights[i] = self.optimizers[weight_optimizer_idx].update(self.weights[i], weight_gradient)
            self.biases[i] = self.optimizers[bias_optimizer_idx].update(self.biases[i], bias_gradient)
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions on new data."""
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        predictions, _ = self.forward(X, training=False)
        return predictions.flatten() if predictions.shape[1] == 1 else predictions
    
    def save(self, filepath: str):
        """Save the trained model."""
        model_data = {
            'weights': self.weights,
            'biases': self.biases,
            'input_size': self.input_size,
            'hidden_sizes': self.hidden_sizes,
            'output_size': self.output_size,
            'activation_name': self.activation_name,
            'output_activation_name': self.output_activation_name,
            'dropout_rate': self.dropout_rate,
            'l1_reg': self.l1_reg,
            'l2_reg': self.l2_reg,
            'is_trained': self.is_trained
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"Advanced model saved to {filepath}")
    
    def load(self, filepath: str):
        """Load a trained model."""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.weights = model_data['weights']
        self.biases = model_data['biases']
        self.input_size = model_data['input_size']
        self.hidden_sizes = model_data['hidden_sizes']
        self.output_size = model_data['output_size']
        self.activation_name = model_data['activation_name']
        self.output_activation_name = model_data['output_activation_name']
        self.dropout_rate = model_data['dropout_rate']
        self.l1_reg = model_data['l1_reg']
        self.l2_reg = model_data['l2_reg']
        self.is_trained = model_data['is_trained']
        
        # Reinitialize activation functions
        self._set_activation_functions()
        
        logger.info(f"Advanced model loaded from {filepath}")


# Export the new model for use
__all__ = ['AdvancedNeuralNetwork', 'ActivationFunction', 'Regularizer', 'SGDOptimizer', 'AdamOptimizer']
